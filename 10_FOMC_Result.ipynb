{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bart For Summarize Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "import torch\n",
    "from transformers import BartForConditionalGeneration, BartTokenizer\n",
    "\n",
    "# Check if GPU is available and set the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load BART model for summarization and move it to the specified device (GPU if available)\n",
    "bart_model = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn').to(device)\n",
    "bart_tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')\n",
    "\n",
    "def split_text(text, max_chunk_size=1024):\n",
    "    tokens = bart_tokenizer.tokenize(text)\n",
    "    chunks = []\n",
    "    for i in range(0, len(tokens), max_chunk_size):\n",
    "        chunk = bart_tokenizer.convert_tokens_to_string(tokens[i:i + max_chunk_size])\n",
    "        chunks.append(chunk)\n",
    "    return chunks\n",
    "\n",
    "def summarize_sentence(text_list):\n",
    "    summaries = []\n",
    "    cur_sentence = ''\n",
    "    for ii, chunk in enumerate(text_list):\n",
    "        if len(cur_sentence+chunk) < 1000 and ii < len(text_list)-1:\n",
    "            if ii == len(text_list)-1:\n",
    "                print('Last Sentence')\n",
    "            cur_sentence = cur_sentence+chunk\n",
    "        else:\n",
    "            inputs = bart_tokenizer([cur_sentence], max_length=1024, return_tensors='pt', truncation=True)\n",
    "\n",
    "            # Move input tensors to the same device as the model\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "            # Generate summary and move it back to CPU for decoding\n",
    "            summary_ids = bart_model.generate(inputs['input_ids'], num_beams=4, max_length=200, early_stopping=True)\n",
    "            summary = bart_tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "            summaries.append(summary)\n",
    "            cur_sentence = chunk\n",
    "    summaries_text = ' '.join(summaries)\n",
    "    if len(summaries_text) <= 1024:\n",
    "        return summaries_text\n",
    "    else:\n",
    "        return summarize_sentence(summaries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LED For summarize text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import LEDTokenizer, LEDForConditionalGeneration\n",
    "import pandas as pd\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load LED model for summarization and move it to the specified device (GPU if available)\n",
    "led_model = LEDForConditionalGeneration.from_pretrained('allenai/led-base-16384').to(device)\n",
    "led_tokenizer = LEDTokenizer.from_pretrained('allenai/led-base-16384')\n",
    "def summarize_sentence_led(text):\n",
    "    paragraphs = text.split('\\n\\n')\n",
    "    print('====Start===')\n",
    "    summaries = []\n",
    "    for paragraph in paragraphs:\n",
    "        text_list = sent_tokenize(paragraph)\n",
    "        cur_sentence = ''\n",
    "        # Predefined settings\n",
    "        max_input_length = 8000\n",
    "        max_summary_length = 800\n",
    "        min_summary_length = 40\n",
    "        length_penalty = 2.0\n",
    "        num_beams = 4\n",
    "        for ii, chunk in enumerate(text_list):\n",
    "            if ii % 1000 == 0:\n",
    "                print(f'Progress: {ii}/{len(text_list)}')\n",
    "            if len(cur_sentence + chunk) < max_input_length - 100 and ii < len(text_list) - 1:\n",
    "                cur_sentence += chunk + ' '\n",
    "            else:\n",
    "                inputs = led_tokenizer(cur_sentence, return_tensors=\"pt\", max_length=max_input_length, truncation=True)\n",
    "                input_ids = inputs.input_ids.to('cuda')\n",
    "                attention_mask = inputs.attention_mask.to('cuda')\n",
    "\n",
    "                # Generate summary\n",
    "                summary_ids = led_model.generate(input_ids, attention_mask=attention_mask, max_length=max_summary_length, min_length=min_summary_length, length_penalty=length_penalty, num_beams=num_beams, early_stopping=True)\n",
    "\n",
    "                # Decode summary\n",
    "                summary = led_tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "                summaries.append(summary)\n",
    "                cur_sentence = chunk\n",
    "        summaries.append('\\n\\n')  # Add a separator for each paragraph summary\n",
    "\n",
    "    summaries_text = ' '.join(summaries)\n",
    "    print(summaries_text)\n",
    "    if len(summaries_text) <= 1024:\n",
    "        print('===Finished===')\n",
    "        return summaries_text\n",
    "    else:\n",
    "        return summarize_sentence_led(summaries_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader,RandomSampler, SequentialSampler,TensorDataset\n",
    "from transformers import BertTokenizer, AdamW ,BertModel, BertPreTrainedModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "\n",
    "class CustomFinBERT(BertPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "\n",
    "        # Load the pre-trained BERT model\n",
    "        self.bert = BertModel(config)\n",
    "\n",
    "        # Define custom feedforward layers\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.fc1 = nn.Linear(config.hidden_size, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, 128)\n",
    "        self.fc4 = nn.Linear(128, 3)  # Output layer for 3 classes\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, token_type_ids=None):\n",
    "        # Get the output from the BERT model\n",
    "        outputs = self.bert(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        sequence_output = outputs[1]  # Use pooled output\n",
    "\n",
    "        # Pass through the custom layers\n",
    "        x = self.dropout(sequence_output)\n",
    "        x = nn.ReLU()(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = nn.ReLU()(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = nn.ReLU()(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "# Prepare the dataset\n",
    "def encode_texts(tokenizer, texts, max_length=512):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    token_type_ids = []\n",
    "\n",
    "    for text in texts:\n",
    "        encoded = tokenizer.encode_plus(\n",
    "            text,\n",
    "            max_length=max_length,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True,\n",
    "            return_token_type_ids=True,  # Include token type ids\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        input_ids.append(encoded['input_ids'])\n",
    "        attention_masks.append(encoded['attention_mask'])\n",
    "        token_type_ids.append(encoded['token_type_ids'])  # Append token type ids\n",
    "\n",
    "    input_ids = torch.cat(input_ids, dim=0)\n",
    "    attention_masks = torch.cat(attention_masks, dim=0)\n",
    "    token_type_ids = torch.cat(token_type_ids, dim=0)  # Concatenate token type ids\n",
    "\n",
    "    return input_ids, attention_masks, token_type_ids\n",
    "\n",
    "def train(model, train_dataloader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in train_dataloader:\n",
    "        # Move batch to GPU\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_token_type_ids, b_labels = batch\n",
    "\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Forward pass, include token_type_ids\n",
    "        outputs = model(b_input_ids, token_type_ids=b_token_type_ids, attention_mask=b_input_mask)\n",
    "        loss = nn.CrossEntropyLoss()(outputs, b_labels)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_dataloader)\n",
    "    return avg_train_loss\n",
    "\n",
    "def evaluate(model, validation_dataloader, device):\n",
    "    model.eval()\n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "\n",
    "    for batch in validation_dataloader:\n",
    "        # Move batch to GPU\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_token_type_ids, b_labels = batch\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Forward pass, include token_type_ids\n",
    "            outputs = model(b_input_ids, token_type_ids=b_token_type_ids, attention_mask=b_input_mask)\n",
    "\n",
    "        loss = nn.CrossEntropyLoss()(outputs, b_labels)\n",
    "        total_eval_loss += loss.item()\n",
    "\n",
    "        preds = torch.argmax(outputs, dim=1).flatten()\n",
    "        total_eval_accuracy += (preds == b_labels).cpu().numpy().mean()\n",
    "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
    "    return avg_val_loss, avg_val_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the Fine-Tuned Model only on FOMC Statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CustomFinBERT(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30873, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (fc1): Linear(in_features=768, out_features=512, bias=True)\n",
       "  (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
       "  (fc3): Linear(in_features=256, out_features=128, bias=True)\n",
       "  (fc4): Linear(in_features=128, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load The pretrained_model\n",
    "model = CustomFinBERT.from_pretrained('models/V1_Epoch75 ')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('yiyanghkust/finbert-pretrain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Data(FOMC + Other Financial sentence)\n",
    "full_df = pd.read_csv('data/ft.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('The sale price was not disclosed .', 0)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Example of Other Sentence\n",
    "ii = 1500\n",
    "text, label = full_df.iloc[ii,1],full_df.iloc[ii,2]\n",
    "text,label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 6.2081, -3.2658, -3.3158]], device='cuda:0',\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test On Model using other Sentence\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "inputs.to(device)\n",
    "outputs = model(**inputs)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Only FOMC Statement\n",
    "validation_text = full_df.iloc[2265:, 1].values\n",
    "validation_label = full_df.iloc[2265:, 2].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A meeting of the Federal Open Market Committee was held in the offices of the Board of Governors of the Federal Reserve System in Washington, D. Madigan and Simpson, Associate Directors, Divisions of '"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_text[0][:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since the FOMC statement are longer, we first encode the text as how we trained the data\n",
    "input_ids, attention_masks, token_type_ids = encode_texts(tokenizer, validation_text)\n",
    "labels = torch.tensor(validation_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the validation set to dataloader\n",
    "validation_data = TensorDataset(input_ids, attention_masks, token_type_ids, labels)\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=294)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.1024499461054802, Validation Accuracy:0.9863945578231292\n"
     ]
    }
   ],
   "source": [
    "# Get The Score\n",
    "avg_val_loss, avg_val_acc = evaluate(model, validation_dataloader, device)\n",
    "print(f\"Validation loss: {avg_val_loss}, Validation Accuracy:{avg_val_acc}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "ml_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
